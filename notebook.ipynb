{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# import keras\n",
    "from keras.models import load_model, Model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, ZeroPadding2D, MaxPooling2D, AveragePooling2D, Dense, Flatten\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file = open(\"dataset/labels.data\", \"r\")\n",
    "LABELS = {name:idx for idx, name in enumerate(labels_file.read().split('\\n'))}\n",
    "\n",
    "labels_file.close()\n",
    "\n",
    "REVERSED_LABELS = {_[0]:_[1] for _ in [(value, key) for key, value in LABELS.items()]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "face_classifier = cv.CascadeClassifier(cv.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def save_bounding_box(img:Image.Image, path:str, shape:tuple):\n",
    "    \n",
    "    img_mat = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)\n",
    "    gray_img = cv.cvtColor(img_mat, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_classifier.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "\n",
    "    faces_to_return = []\n",
    "    root_pos = path.rfind('.')\n",
    "    index = 0\n",
    "    for (x, y, w, h) in faces:\n",
    "\n",
    "        to_save = cv.resize(img_mat[y:y+h, x:x+w], shape)\n",
    "\n",
    "        cv.imwrite(\n",
    "           path[:root_pos] + str(index) + path[root_pos:],\n",
    "           to_save\n",
    "        )\n",
    "        index += 1\n",
    "\n",
    "        faces_to_return.append(Image.fromarray(cv.cvtColor(to_save, cv.COLOR_BGR2RGB)))\n",
    "\n",
    "    return faces_to_return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load(dir:str, shape:tuple=(224,224)) -> tuple:\n",
    "\n",
    "    # Loading dataset\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    dir_content = os.listdir(dir)\n",
    "\n",
    "    for person_folder in dir_content: # For each person's folder\n",
    "    \n",
    "        for _ in os.listdir(os.path.join(dir, person_folder)): # For face image in a person's folder\n",
    "            \n",
    "            if '.' in _: # If _ is actually a file\n",
    "                file = Image.open(os.path.join(dir, person_folder, _))\n",
    "\n",
    "                # If the loaded image doesn't meet the shape standards (maybe not cropped yet) we do so,\n",
    "                # save the cropped version before adding to the dataset\n",
    "                if file.size != shape:\n",
    "                    # faces is a list consisting of Image objects of all the faces extrated in the current file \n",
    "                    faces = save_bounding_box(file, os.path.join(dir, _), shape)\n",
    "\n",
    "                    # Adding the face(s)\n",
    "                    for f in faces:\n",
    "\n",
    "                        data.append(np.asarray(f))\n",
    "                    \n",
    "                        # Adding the label\n",
    "                        for key in LABELS.keys():\n",
    "                            if key in _:\n",
    "                                labels.append(LABELS[key])\n",
    "                                break\n",
    "\n",
    "                    # Moving the old parent image\n",
    "                    os.system(\"mkdir \" + os.path.join(dir, \"old_images\").replace('/', '\\\\'))\n",
    "                    # print(('move \"' + os.path.join(dir, _) + '\" \"' + os.path.join(dir, \"old_images/\")).replace('/', '\\\\') + '\"')\n",
    "                    os.system(('move \"' + os.path.join(dir, _) + '\" \"' + os.path.join(dir, \"old_images/\")).replace('/', '\\\\') + '\"')\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    # Adding the file\n",
    "                    data.append(np.asarray(file))\n",
    "                    \n",
    "                    # Adding the label\n",
    "                    for key in LABELS.keys():\n",
    "                        if key.lower() == person_folder.lower() or person_folder.lower() in key.lower():\n",
    "                            labels.append(LABELS[key])\n",
    "                            break\n",
    "\n",
    "\n",
    "    return np.array(data), to_categorical(labels)\n",
    "\n",
    "\n",
    "def load_data(path:str=\"./dataset\") -> tuple:\n",
    "\n",
    "    train_data_path = os.path.join(path, \"train_data\")\n",
    "    test_data_path = os.path.join(path, \"test_data\")\n",
    "\n",
    "    # Loading training dataset\n",
    "    train_data = load(train_data_path)\n",
    "    # Loading training dataset\n",
    "    test_data = load(test_data_path)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TimingCallback(Callback):\n",
    "    \n",
    "    def __init__(self, logs={}):\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.logs.append(timer() - self.starttime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation, loading and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implementation of convolution block\n",
    "def convolutional_block(X, f, filters, stage, block, s):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "#Implementation of Identity Block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Skip Connection\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Implementation of ResNet-50\n",
    "def ResNet50(input_shape=(224, 224, 3)):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[16, 16, 64], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='c')\n",
    "\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[32, 32, 128], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='d')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='f')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=5, block='c')\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def create_model() -> Model:\n",
    "\n",
    "    base_model = ResNet50(input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', name='fc1',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "    x = Dense(256, activation='relu', name='fc2',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "    x = Dense(3, activation='softmax', name='fc3',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    # for layer in base_model.layers:\n",
    "    #     layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(model:Model):\n",
    "\n",
    "    model.save(\"base_model_og.keras\")\n",
    "\n",
    "def load_weights():\n",
    "\n",
    "    return load_model(\"base_model_og.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nLoading the model ...')\n",
    "model = create_model()\n",
    "\n",
    "print(\"\\nLoading the model's weights ...\")\n",
    "model.set_weights(load_weights())\n",
    "\n",
    "print('\\nLoading the dataset ...')\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "print('\\nCompiling the model ...')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining the model ...')\n",
    "# callback = EarlyStopping(monitor=\"val_accuracy\", patience=1)\n",
    "\n",
    "filepath = \"model-{epoch:.2f}-loss-{loss:.2f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, shuffle=True, callbacks=[callbacks_list])\n",
    "\n",
    "print(\"\\nEnd of training.\")\n",
    "\n",
    "print(\"\\nSaving...\")\n",
    "save_model(model)\n",
    "\n",
    "print('\\nModel saved.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
