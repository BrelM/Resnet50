{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# import keras\n",
    "from keras.models import load_model, Model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, ZeroPadding2D, MaxPooling2D, AveragePooling2D, Dense, Flatten\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are hardcoded for test purpose only, not for production intends\n",
    "\n",
    "LABELS = {\n",
    "    \"Brel\":0,\n",
    "    \"Anselme\":1,\n",
    "    \"Rafiatou\":2\n",
    "}\n",
    "\n",
    "REVERSED_LABELS = {_[0]:_[1] for _ in [(value, key) for key, value in LABELS.items()]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(dir:str, shape:tuple=(224,224, 3)) -> tuple:\n",
    "\n",
    "    # Loading dataset\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    dir_content = os.listdir(dir)\n",
    "\n",
    "    for _ in dir_content:\n",
    "        file = Image.open(os.path.join(dir, _)).resize((shape[0], shape[1]))\n",
    "\n",
    "        temp_file = np.asarray(file)\n",
    "        if file.size == (224, 224):\n",
    "            file.save(os.path.join(dir, _))\n",
    "            file = temp_file\n",
    "        else:\n",
    "            file = temp_file.reshape((1, shape[0], shape[1], shape[2]))\n",
    "\n",
    "        # Adding the file\n",
    "        data.append(file)\n",
    "        \n",
    "        # Adding the label\n",
    "        for key in LABELS.keys():\n",
    "            if key in _:\n",
    "                labels.append(LABELS[key])\n",
    "                break\n",
    "    \n",
    "    return np.array(data), to_categorical(labels)\n",
    "\n",
    "\n",
    "def load_data(path:str=\"./dataset\") -> tuple:\n",
    "\n",
    "    train_data_path = os.path.join(path, \"train_data\")\n",
    "    test_data_path = os.path.join(path, \"test_data\")\n",
    "\n",
    "    # Loading training dataset\n",
    "    train_data = load(train_data_path)\n",
    "    # Loading training dataset\n",
    "    test_data = load(test_data_path)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model implementation, loading and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implementation of convolution block\n",
    "def convolutional_block(X, f, filters, stage, block, s):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "#Implementation of Identity Block\n",
    "\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Skip Connection\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Implementation of ResNet-50\n",
    "def ResNet50(input_shape=(224, 224, 3)):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[16, 16, 64], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [16, 16, 64], stage=2, block='c')\n",
    "\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[32, 32, 128], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [32, 32, 128], stage=3, block='d')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=4, block='f')\n",
    "\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=5, block='c')\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# @keras.saving.get_custom_objects().clear()\n",
    "\n",
    "# # Creatig a proper customr ResNet50 layer for serialization\n",
    "# @keras.saving.register_keras_serializable()\n",
    "# class ResNet50Class(keras.layers.Layer):\n",
    "    \n",
    "#     def __init__(self, nested_model:Model=None):\n",
    "#         super().__init__()\n",
    "#         self.nested_model = nested_model\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         # Updation of config with custom layer's parameters\n",
    "#         for layer in self.nested_model.layers:\n",
    "#             config.update(\n",
    "#                 {\n",
    "#                     layer.name: None\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         return config\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_config(cls, config):\n",
    "#         return cls(**config)\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         return self.nested_model(inputs)\n",
    "    \n",
    "\n",
    "    \n",
    "def create_model() -> Model:\n",
    "\n",
    "    # base_model = ResNet50Class(ResNet50(input_shape=(224, 224, 3))())\n",
    "    # x = base_model.nested_model.output\n",
    "    base_model = ResNet50(input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', name='fc1',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "    x = Dense(256, activation='relu', name='fc2',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "    x = Dense(3, activation='softmax', name='fc3',kernel_initializer=glorot_uniform(seed=0))(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model(model:Model):\n",
    "    \n",
    "    with open(\"base_model.keras\", \"wb\") as model_file:\n",
    "        pickle.dump(model.get_weights(), model_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# model.save(\"base_model.keras\")\n",
    "\n",
    "def load_weights():\n",
    "    \n",
    "    with open(\"base_model1.keras\", \"rb\") as model_file:\n",
    "        return pickle.load(model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nLoading the model ...')\n",
    "model = create_model()\n",
    "\n",
    "print(\"\\nLoading the model's weights ...\")\n",
    "model.set_weights(load_weights())\n",
    "\n",
    "print('\\nLoading the dataset ...')\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "print('\\nCompiling the model ...')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('\\nTraining the model ...')\n",
    "# callback = EarlyStopping(monitor=\"val_accuracy\", patience=1)\n",
    "\n",
    "filepath = \"model-{epoch:.2f}-loss-{loss:.2f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, shuffle=True, callbacks=[callbacks_list])\n",
    "\n",
    "print(\"\\nEnd of training.\")\n",
    "\n",
    "print(\"\\nSaving...\")\n",
    "save_model(model)\n",
    "\n",
    "print('\\nModel saved.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
